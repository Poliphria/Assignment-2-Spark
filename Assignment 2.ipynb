{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to parse xml files. Convert to csv to make easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users done\n",
      "post history done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "    \n",
    "def parsexml(xmlFile, fileName):\n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # open csv file to write to\n",
    "    data = open(fileName, 'w')\n",
    "    csvwriter = csv.writer(data)\n",
    "\n",
    "    # get keys from object \n",
    "    head = list(root[0].attrib.keys())\n",
    "    csvwriter.writerow(head)\n",
    "    \n",
    "    # write each line to csv file\n",
    "    for line in root.findall('row'):\n",
    "        lineValues = []\n",
    "        for key in head:\n",
    "            #if key doesn't exist, just put empty for that key's value\n",
    "            lineValues.append(line.attrib.get(key, 'empty'))\n",
    "        csvwriter.writerow(lineValues)\n",
    "            \n",
    "\n",
    "def main():\n",
    "    parsexml('./Users.xml', 'users.csv')\n",
    "    print('users done')\n",
    "    parsexml('./PostHistory.xml', 'posthistory.csv')\n",
    "    print('post history done')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Users.xml file, find all users which are from Georgia and output to screen their\n",
    "DisplayName only. (In this case I'm loading the data from the Users csv created above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the names of everyone that lives in Georgia. This does not include the country of Georgia:\n",
      "\n",
      "Ayush, vkb, Dan Anton, Manish Ranjan, Dato Janez, Neuromeda, Jenna Kwon, Jeff, Jesse Scherer, Melinda Weathers, N.C.W., Tae-Sung Shin, Clark Lin, alerera, Max Boonbandansook, Tony Boyles, pkerl, Nick Larsen, gfritz, Aleksandr Blekh, Michael, azoorob, ontek, Aravind R. Yarram, ilya, Daisuke Aramaki, tempusfugit, Henry Crutcher, Goddard, Matt Simpson, Peter Woolfitt, matt biskup, Jason W, Peter Mourfield, Magsol, Bob Baxley, badjr, mplunney, YC Hu, ryan, Patrick Gerbes, Ilya Lapitan, pradyumnad, Psidom, Teresa Madsen, Brandon, jpm, Mr. Rooter of Savannah, Mr. Rooter of Southeast GA, Khiem Ha, Ahmet Cecen, Guy Gordon, C3Theo, niru dyogi, Vinitha Palani, Mac18, Andrew, Aditya Gogoi, turtlemonvh, Lewis Rodgers, Tarun Luthra, Devendra Lattu, cosmosa, Todd Dawson, ironv, Mboolean, Jimd, David F, PSInf, Chirag, Sandeep Gunda, hellofanengineer, Will Gao, Oriol Mirosa, Andrew King, David, Nghia.xlee, rajb245, Sealander, afshin, Ashish Powani, Boris N., Atul Kaushik, Harnoor Singh, Vincent, Tiago Cogumbreiro, cbarrick, Len Greski, red_eight, PEBKAC, Christoph, Bryce, David Hofmann, nburn42, donlan, Nick M, Kiran, BarclayK, Zer0k, dportman, Shishir Suman, Scott, JessicaRabi, Baxter, Rama Ananda, Dr. Strange, Shahan M, wgreenihrcorp, zongyan, Alexandre, rams, John Barbour, The_Flin, DaulPavid, Amandeep Jiddewar, Erica Rosa, user1410665, empoleon, Willie-G, Deepak Shenoy, Ragnar Lothbrok, Deontaé Le Pew, Doctorambient, Vidya, Vatsal Srivastava, Dean Webb, Sam Washburn, Loisaida Sam Sandberg, wayne green, gnorman, Floydian, Meng Zhao, Manikanta Reddy D, devarsh raghnathbhai patel, Myron Slaw, rmooney, mike_stevs, Gary Lai, ajroot, Daniel, phos, Lance Ruo Zhang, Spencer-Price, Stephen Ewing, alerera, addi wei, Alex V, Rob, Rajan, Ram, jGaboardi, zachdj, David Zhou, ps0604, treysp, Eduardo Trunci, Squ1rr3lz, Tiji Mathew, user3776637\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"assignment2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rdd = spark.read.csv('./users.csv', header=True).rdd\n",
    "people = rdd.map(lambda x: (x[3],x[6]))\n",
    "people = people.filter(lambda x: x[1]!=None)\n",
    "people = people.filter(lambda x: x[1]!='empty')\n",
    "people = people.map(lambda x: (x[0],x[1].lower()))\n",
    "full = people.filter(lambda data: \"georgia\" in data[1])\n",
    "full = full.filter(lambda data: \"tbilisi\" not in data[1])\n",
    "full = full.filter(lambda data: \"rustavi\" not in data[1])\n",
    "abbrev = people.filter(lambda data: re.findall('\\\\bga\\\\b',data[1]))\n",
    "georgiaPeople = full.union(abbrev)\n",
    "georgiaPeople = georgiaPeople.map(lambda x: x[0])\n",
    "georgiaPeople = georgiaPeople.collect()\n",
    "\n",
    "print(\"These are the names of everyone that lives in Georgia. This does not include the country of Georgia:\\n\")\n",
    "print(*georgiaPeople, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Users.xml file, provide the count for all users which joined (CreationDate) in 2017. (30\n",
    "points). Output this to the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of people that created their account in 2017 is: 14239\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"assignment2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rdd = spark.read.csv('./users.csv', header=True).rdd\n",
    "dates = rdd.map(lambda x: (x[2],1))\n",
    "dates = dates.filter(lambda x: x[0] != None)\n",
    "dates = dates.filter(lambda x: \"2017\" in x[0])\n",
    "dates = dates.filter(lambda x: x[0][0].isdigit())\n",
    "dates = dates.map(lambda x: (x[0][0:4], 1))\n",
    "dates = dates.reduceByKey(lambda x,y: x + y)\n",
    "count = dates.collect()\n",
    "\n",
    "print(\"The number of people that created their account in 2017 is: \" + str(count[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PostHistory file, count the number of Posts that feature the words “Spark” and\n",
    "“Scala”. Output this to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of posts that feature word 'scala': 124\n",
      "Total number of posts that feature word 'spark': 739\n",
      "Total number of posts that feature both words: 52\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"assignment2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rdd = spark.read.csv('./posthistory.csv', header=True).rdd\n",
    "text = rdd.map(lambda x: x[6])\n",
    "text = text.filter(lambda x: x != None)\n",
    "text = text.map(lambda x: x.lower())\n",
    "sparkText = text.filter(lambda line: re.findall('\\\\bspark\\\\b', line))\n",
    "scalaText = text.filter(lambda line: re.findall('\\\\bscala\\\\b', line))\n",
    "\n",
    "print(\"Total number of posts that feature word 'scala': \" + str(scalaText.count()))\n",
    "print(\"Total number of posts that feature word 'spark': \" + str(sparkText.count()))\n",
    "\n",
    "both = sparkText.intersection(scalaText).count()\n",
    "\n",
    "print(\"Total number of posts that feature both words: \" + str(both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PostHistory file, provide a total count of the words used by each distinct user. In other\n",
    "words, count all words in all posts for each user and display this to screen. You can only identify\n",
    "users by the UserID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"assignment2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rdd = spark.read.csv('./posthistory.csv', header=True).rdd\n",
    "wordT = rdd.map(lambda x: (x[5], x[6]))\n",
    "wordT = wordT.filter(lambda x: x[1] != None)\n",
    "wordT = wordT.map(lambda x: (x[0], x[1].lower()))\n",
    "wordT = wordT.map(lambda x: (x[0], x[1].split(\" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
